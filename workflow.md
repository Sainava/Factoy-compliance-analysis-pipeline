# Visual Workflow: Nodes and Edges

This file describes the pipeline strictly as nodes and edges. It contains:
- A single Mermaid flowchart showing the DAG
- A plain adjacency list for clarity

---

## Graph

```mermaid
flowchart TD
    A[Video Input] --> B[Preprocess]
    B --> C[Detection]
    C --> D[Tracking]
    D --> E[Pose]
    E --> F[Rules]
    F --> G[Temporal]
    G --> H[Evidence]
    H --> I[Scoring]
    I --> J[Output]

    style A fill:#e3f2fd
    style B fill:#e8f5e8
    style C fill:#e3f2fd
    style D fill:#e8f5e8
    style E fill:#e3f2fd
    style F fill:#e8f5e8
    style G fill:#e3f2fd
    style H fill:#e8f5e8
    style I fill:#e3f2fd
    style J fill:#e8f5e8
```

---

## Adjacency List

```
A -> B
B -> C
C -> D
D -> E
E -> F
F -> G
G -> H
H -> I
I -> J
```

---

Legend
- Detection = YOLOv8
- Tracking = DeepSORT
- Pose = MediaPipe
- Rules = Rule Engine
- Temporal = Temporal reasoning
- Evidence = Evidence generation H264 avc1
- Output = JSON and UI
# Compliance Analysis Workflow (Visual)# Compliance Analysis Workflow (Visual)



All visuals are Mermaid diagrams with a blue/green theme. No emojis. Labels are simplified to avoid parser issues.All visuals below are Mermaid diagrams (emoji-free) using a strict blue/green palette.



------



## 1) End-to-End Pipeline## 1) End-to-End Pipeline



```mermaid```mermaid

flowchart TDflowchart TD

    A[Video Input] --> B[Preprocess]    A[Video Input] --> B[Preprocess]

    B --> C[YOLOv8 Detection]    B --> C[YOLOv8 Detection]

    C --> D[DeepSORT Tracking]    C --> D[DeepSORT Tracking]

    D --> E[MediaPipe Pose]    D --> E[MediaPipe Pose]

    E --> F[Rule Engine]    E --> F[Rule Engine]

    F --> G[Temporal Reasoning]    F --> G[Temporal Reasoning]

    G --> H[Evidence Generation H264 avc1]    G --> H[Evidence Generation (H.264/avc1)]

    H --> I[Compliance Scoring]    H --> I[Compliance Scoring]

    I --> J[JSON Output and UI]    I --> J[JSON Output / UI]



    style A fill:#e3f2fd    style A fill:#e3f2fd

    style B fill:#e8f5e8    style B fill:#e8f5e8

    style C fill:#e3f2fd    style C fill:#e3f2fd

    style D fill:#e8f5e8    style D fill:#e8f5e8

    style E fill:#e3f2fd    style E fill:#e3f2fd

    style F fill:#e8f5e8    style F fill:#e8f5e8

    style G fill:#e3f2fd    style G fill:#e3f2fd

    style H fill:#e8f5e8    style H fill:#e8f5e8

    style I fill:#e3f2fd    style I fill:#e3f2fd

    style J fill:#e8f5e8    style J fill:#e8f5e8

``````



------



## 2) Video Input and Preprocessing## 2) Video Input & Preprocessing



```mermaid```mermaid

flowchart LRflowchart LR

    A[Standard Video MP4 or MOV] --> C[Resize 640x640]    A[Standard Video (MP4/MOV)] --> C[Resize 640x640]

    B[360 Video Equirectangular] --> D[Generate Perspective Tiles]    B[360° Video (Equirectangular)] --> D[Generate Perspective Tiles]

    C --> E[Sample 2 FPS]    C --> E[Sample 2 FPS]

    D --> E    D --> E

    E --> F[Frames for Inference]    E --> F[Frames for Inference]



    style A fill:#e3f2fd    style A fill:#e3f2fd

    style B fill:#e8f5e8    style B fill:#e8f5e8

    style C fill:#e3f2fd    style C fill:#e3f2fd

    style D fill:#e8f5e8    style D fill:#e8f5e8

    style E fill:#e3f2fd    style E fill:#e3f2fd

    style F fill:#e8f5e8    style F fill:#e8f5e8

``````



------



## 3) Per Frame Intelligence## 3) Per-Frame Intelligence



```mermaid```mermaid

flowchart TDflowchart TD

    A[Frame 640x640] --> B[YOLOv8 Detect Persons PPE Hazards]    A[640x640 Frame] --> B[YOLOv8: Persons / PPE / Hazards]

    B --> C[Detections]    B --> C[Detections]

    C --> D[DeepSORT Assign Track IDs]    C --> D[DeepSORT: Track IDs]

    D --> E[MediaPipe Pose]    D --> E[MediaPipe Pose]

    E --> F[Postures and Motions]    E --> F[Postures / Motions]



    style A fill:#e3f2fd    style A fill:#e3f2fd

    style B fill:#e8f5e8    style B fill:#e8f5e8

    style C fill:#e3f2fd    style C fill:#e3f2fd

    style D fill:#e8f5e8    style D fill:#e8f5e8

    style E fill:#e3f2fd    style E fill:#e3f2fd

    style F fill:#e8f5e8    style F fill:#e8f5e8

``````



------



## 4) Rules Evidence and Scoring## 4) Rules, Evidence, and Scoring



```mermaid```mermaid

flowchart TDflowchart TD

    A[Person Timelines] --> B[Rule Engine]    A[Person Timelines] --> B[Rule Engine]

    B --> C[Violation Events]    B --> C[Violation Events]

    C --> D[Evidence Clips and Thumbnails]    C --> D[Evidence Clips + Thumbnails]

    D --> E[Encode H264 avc1]    D --> E[Encode H.264/avc1]

    E --> F[Persist to outputs]    E --> F[Persist to /outputs]



    B --> G[Compliance Metrics]    B --> G[Compliance Metrics]

    G --> H[Score 100 minus violations over total times 100]    G --> H[Score = 100 - (violations / total) * 100]



    style A fill:#e3f2fd    style A fill:#e3f2fd

    style B fill:#e8f5e8    style B fill:#e8f5e8

    style C fill:#e3f2fd    style C fill:#e3f2fd

    style D fill:#e8f5e8    style D fill:#e8f5e8

    style E fill:#e3f2fd    style E fill:#e3f2fd

    style F fill:#e8f5e8    style F fill:#e8f5e8

    style G fill:#e3f2fd    style G fill:#e3f2fd

    style H fill:#e8f5e8    style H fill:#e8f5e8

``````



------



## 5) System Architecture## 5) System Architecture



```mermaid```mermaid

flowchart LRflowchart LR

    A[Upload API FastAPI] --> B[Pipeline Worker]    A[Upload API (FastAPI)] --> B[Pipeline Worker]

    B --> C[Preprocess and Inference]    B --> C[Preprocess + Inference]

    C --> D[Evidence and Scoring]    C --> D[Evidence & Scoring]

    D --> E[Results Store]    D --> E[Results Store]

    E --> F[Web UI Dashboard]    E --> F[Web UI / Dashboard]

    F --> G[Serve Evidence Static Files]    F --> G[Serve Evidence (Static Files)]



    style A fill:#e3f2fd    style A fill:#e3f2fd

    style B fill:#e8f5e8    style B fill:#e8f5e8

    style C fill:#e3f2fd    style C fill:#e3f2fd

    style D fill:#e8f5e8    style D fill:#e8f5e8

    style E fill:#e3f2fd    style E fill:#e3f2fd

    style F fill:#e8f5e8    style F fill:#e8f5e8

    style G fill:#e3f2fd    style G fill:#e3f2fd

``````



------



NotesNotes

- Strict blue #e3f2fd and green #e8f5e8 only.- Emoji-free; colors limited to blue (#e3f2fd) and green (#e8f5e8).

- Evidence videos encoded H264 avc1 for browser playback.- Evidence videos encoded H.264/avc1 for browser playback.

- Should render in VS Code Markdown Preview and GitHub.- Should render in VS Code Markdown Preview and GitHub.

# Compliance Analysis Workflow (Visual)

This document provides visual diagrams of the end-to-end pipeline. All diagrams use a blue/green theme and no emojis.

---

## 1) End-to-End Pipeline

```mermaid
flowchart TD
    A[Video Input] --> B[Preprocess]
    B --> C[YOLOv8 Detection]
    C --> D[DeepSORT Tracking]
    D --> E[MediaPipe Pose]
    E --> F[Rule Engine]
    F --> G[Temporal Reasoning]
    G --> H[Evidence Generation (H.264/avc1)]
    H --> I[Compliance Scoring]
    I --> J[JSON Output / UI]

    style A fill:#e3f2fd
    style B fill:#e8f5e8
    style C fill:#e3f2fd
    style D fill:#e8f5e8
    style E fill:#e3f2fd
    style F fill:#e8f5e8
    style G fill:#e3f2fd
    style H fill:#e8f5e8
    style I fill:#e3f2fd
    style J fill:#e8f5e8
```

---

## 2) Video Input & Preprocessing

```mermaid
flowchart LR
    A[Standard Video (MP4/MOV)] --> C[Resize 640x640]
    B[360° Video (Equirectangular)] --> D[Generate Perspective Tiles]
    C --> E[Sample 2 FPS]
    D --> E
    E --> F[Frames for Inference]

    style A fill:#e3f2fd
    style B fill:#e8f5e8
    style C fill:#e3f2fd
    style D fill:#e8f5e8
    style E fill:#e3f2fd
    style F fill:#e8f5e8
```

---

## 3) Per-Frame Intelligence

```mermaid
flowchart TD
    A[640x640 Frame] --> B[YOLOv8: Persons / PPE / Hazards]
    B --> C[Detections (bboxes, classes, conf)]
    C --> D[DeepSORT: Associate + Track IDs]
    D --> E[MediaPipe Pose: 33 keypoints]
    E --> F[Derived Postures / Motions]

    style A fill:#e3f2fd
    style B fill:#e8f5e8
    style C fill:#e3f2fd
    style D fill:#e8f5e8
    style E fill:#e3f2fd
    style F fill:#e8f5e8
```

---

## 4) Rules, Evidence, and Scoring

```mermaid
flowchart TD
    A[Person Timelines] --> B[Rule Engine]
    B --> C[Violation Events]
    C --> D[Evidence Clips + Thumbnails]
    D --> E[Encode H.264/avc1]
    E --> F[Persist to /outputs]

    B --> G[Compliance Metrics]
    G --> H[Score = 100 - (violations / total) * 100]

    style A fill:#e3f2fd
    style B fill:#e8f5e8
    style C fill:#e3f2fd
    style D fill:#e8f5e8
    style E fill:#e3f2fd
    style F fill:#e8f5e8
    style G fill:#e3f2fd
    style H fill:#e8f5e8
```

---

## 5) System Architecture

```mermaid
flowchart LR
    A[Upload API (FastAPI)] --> B[Pipeline Worker]
    B --> C[Preprocess + Inference]
    C --> D[Evidence & Scoring]
    D --> E[Results Store]
    E --> F[Web UI / Dashboard]
    F --> G[Serve Evidence (Static Files)]

    style A fill:#e3f2fd
    style B fill:#e8f5e8
    style C fill:#e3f2fd
    style D fill:#e8f5e8
    style E fill:#e3f2fd
    style F fill:#e8f5e8
    style G fill:#e3f2fd
```

---

Notes
- All diagrams are emoji-free and use only blue (#e3f2fd) and green (#e8f5e8).
- Evidence videos are encoded as H.264/avc1 for browser playback.
- Visuals render in GitHub and in VS Code Markdown Preview.
# Compliance Detection Model Architecture# Compliance Detection Model Architecture# Compliance Detection Model Architecture# Compliance Detection Model Architecture



This is the ML model pipeline for analyzing factory videos and detecting compliance violations. It's designed to be modular, scalable, and ready for integration.



---This is the ML model pipeline for analyzing factory videos and detecting compliance violations. It's designed to be modular, scalable, and ready for integration.



## Complete Pipeline Flow



```mermaid---This is the ML model pipeline for analyzing factory videos and detecting compliance violations. It's designed to be modular, scalable, and ready for integration.This is the ML model pipeline for analyzing factory videos and detecting compliance violations. It's designed to be modular, scalable, and ready for integration.

flowchart TD

    A[Video Input MP4/360°] --> B{Video Type?}

    B -->|Standard| C[Resize to 640x640]

    B -->|360°| D[Convert to Perspective Tiles]## Complete Pipeline Flow

    C --> E[Frame Sampling 2 FPS]

    D --> E

    E --> F[YOLOv8 Object Detection]

    F --> G[DeepSORT Tracking]```mermaid------

    G --> H[MediaPipe Pose Analysis]

    H --> I[Rule Engine Evaluation]flowchart TD

    I --> J[Temporal Reasoning]

    J --> K[Evidence Generation]    A[Video Input MP4/360°] --> B{Video Type?}

    K --> L[Compliance Scoring]

    L --> M[JSON Output Dashboard Ready]    B -->|Standard| C[Resize to 640x640]

    

    style A fill:#e3f2fd    B -->|360°| D[Convert to Perspective Tiles]## Complete Pipeline Flow## Complete Pipeline Flow

    style C fill:#e8f5e8

    style D fill:#e3f2fd    C --> E[Frame Sampling 2 FPS]

    style E fill:#e8f5e8

    style F fill:#e3f2fd    D --> E

    style G fill:#e8f5e8

    style H fill:#e3f2fd    E --> F[YOLOv8 Object Detection]

    style I fill:#e8f5e8

    style J fill:#e3f2fd    F --> G[DeepSORT Tracking]```mermaid```mermaid

    style K fill:#e8f5e8

    style L fill:#e3f2fd    G --> H[MediaPipe Pose Analysis]

    style M fill:#e8f5e8

```    H --> I[Rule Engine Evaluation]flowchart TDflowchart TD



---    I --> J[Temporal Reasoning]



## Video Input & Preprocessing    J --> K[Evidence Generation]    A[Video Input MP4/360°] --> B{Video Type?}    A[Video Input MP4/360°] --> B{Video Type?}



```mermaid    K --> L[Compliance Scoring]

flowchart LR

    A[Standard Video MP4/MOV] --> C[Resize 640x640]    L --> M[JSON Output Dashboard Ready]    B -->|Standard| C[Resize to 640x640]    B -->|Standard| C[Resize to 640x640]

    B[360° Video Equirectangular] --> D[Generate Tiles 4 perspectives]

        

    C --> E[Frame Sampling Every 0.5s]

    D --> F[Per-tile Sampling Every 0.5s]    style A fill:#e3f2fd    B -->|360°| D[Convert to Perspective Tiles]    B -->|360°| D[Convert to Perspective Tiles]

    

    E --> G[Ready for Detection]    style C fill:#e8f5e8

    F --> G

        style D fill:#e3f2fd    C --> E[Frame Sampling 2 FPS]    C --> E[Frame Sampling 2 FPS]

    style A fill:#e3f2fd

    style B fill:#e8f5e8    style E fill:#e8f5e8

    style C fill:#e3f2fd

    style D fill:#e8f5e8    style F fill:#e3f2fd    D --> E    D --> E

    style E fill:#e3f2fd

    style F fill:#e8f5e8    style G fill:#e8f5e8

    style G fill:#e3f2fd

```    style H fill:#e3f2fd    E --> F[YOLOv8 Object Detection]    E --> F[YOLOv8 Object Detection]



**Purpose**: Normalize video input for consistent ML processing across different camera types.    style I fill:#e8f5e8



**Input Types**:    style J fill:#e3f2fd    F --> G[DeepSORT Tracking]    F --> G[DeepSORT Tracking]

- Standard MP4/MOV walkthrough videos (phone/CCTV)

- 360° factory surveillance videos (equirectangular format)    style K fill:#e8f5e8



```python    style L fill:#e3f2fd    G --> H[MediaPipe Pose Analysis]    G --> H[MediaPipe Pose Analysis]

# Load and sample video frames at 2 FPS

cap = cv2.VideoCapture(video_path)    style M fill:#e8f5e8

sample_interval = int(fps / 2)  # Sample every 0.5 seconds

```    H --> I[Rule Engine Evaluation]    H --> I[Rule Engine Evaluation]

for frame_idx in range(0, total_frames, sample_interval):

    ret, frame = cap.read()

    if ret:

        frame = cv2.resize(frame, (640, 640))  # YOLOv8 input size---    I --> J[Temporal Reasoning]    I --> J[Temporal Reasoning]

        frame = frame / 255.0  # Normalize to [0,1]

        frames.append((frame_idx, timestamp, frame))

```

## 1. Video Input & Preprocessing    J --> K[Evidence Generation]    J --> K[Evidence Generation]

---



## Object Detection Layer (YOLOv8)

**Purpose**: Normalize video input for consistent ML processing across different camera types.    K --> L[Compliance Scoring]    K --> L[Compliance Scoring]

```mermaid

flowchart TD

    A[640x640 Frame] --> B[YOLOv8 Model]

    B --> C[Person Detection]**Input Types**:    L --> M[JSON Output Dashboard Ready]    L --> M[JSON Output Dashboard Ready]

    B --> D[PPE Detection]

    B --> E[Hazard Detection]- Standard MP4/MOV walkthrough videos (phone/CCTV)

    B --> F[Equipment Detection]

    - 360° factory surveillance videos (equirectangular format)        

    C --> G[Bounding Boxes]

    D --> G

    E --> G

    F --> G```python    style A fill:#e3f2fd    style A fill:#e3f2fd

    

    G --> H[Confidence Filtering >0.45]# Load and sample video frames at 2 FPS

    H --> I[NMS IoU <0.5]

    I --> J[Final Detections]cap = cv2.VideoCapture(video_path)    style C fill:#e8f5e8    style C fill:#e8f5e8

    

    style A fill:#e3f2fdsample_interval = int(fps / 2)  # Sample every 0.5 seconds

    style B fill:#e8f5e8

    style C fill:#e3f2fd    style D fill:#e3f2fd    style D fill:#e3f2fd

    style D fill:#e8f5e8

    style E fill:#e3f2fdfor frame_idx in range(0, total_frames, sample_interval):

    style F fill:#e8f5e8

    style G fill:#e3f2fd    ret, frame = cap.read()    style E fill:#e8f5e8    style E fill:#e8f5e8

    style H fill:#e8f5e8

    style I fill:#e3f2fd    if ret:

    style J fill:#e8f5e8

```        frame = cv2.resize(frame, (640, 640))  # YOLOv8 input size    style F fill:#e3f2fd    style F fill:#e3f2fd



**Purpose**: Identify people, PPE, equipment, hazards, and safety infrastructure.        frame = frame / 255.0  # Normalize to [0,1]



**Custom Detection Classes**:        frames.append((frame_idx, timestamp, frame))    style G fill:#e8f5e8    style G fill:#e8f5e8

```yaml

names:```

  0: person

  1: helmet    style H fill:#e3f2fd    style H fill:#e3f2fd

  2: vest  

  3: gloves---

  4: mask

  5: exit_sign    style I fill:#e8f5e8    style I fill:#e8f5e8

  6: fire_extinguisher

  7: spill## 2. Object Detection Layer (YOLOv8)

  8: obstruction

  9: machinery    style J fill:#e3f2fd    style J fill:#e3f2fd

```

**Purpose**: Identify people, PPE, equipment, hazards, and safety infrastructure.

---

    style K fill:#e8f5e8    style K fill:#e8f5e8

## Tracking Layer (DeepSORT)

**Custom Detection Classes**:

```mermaid

flowchart TD```yaml    style L fill:#e3f2fd    style L fill:#e3f2fd

    A[Frame N Detections] --> B[DeepSORT Tracker]

    B --> C[Track Association]names:

    C --> D[Kalman Filter Prediction]

    D --> E[Feature Matching]  0: person    style M fill:#e8f5e8    style M fill:#e8f5e8

    E --> F[Track ID Assignment]

      1: helmet

    F --> G[Person Timeline]

    G --> H[Track ID: person_1]  2: vest  ``````

    G --> I[Track ID: person_2] 

    G --> J[Track ID: person_N]  3: gloves

    

    H --> K[Frame-by-frame History]  4: mask

    I --> K

    J --> K  5: exit_sign

    

    style A fill:#e3f2fd  6: fire_extinguisher------

    style B fill:#e8f5e8

    style C fill:#e3f2fd  7: spill

    style D fill:#e8f5e8

    style E fill:#e3f2fd  8: obstruction

    style F fill:#e8f5e8

    style G fill:#e3f2fd  9: machinery

    style H fill:#e8f5e8

    style I fill:#e3f2fd```## 1. Video Input & Preprocessing## 1. Video Input & Preprocessing

    style J fill:#e8f5e8

    style K fill:#e3f2fd

```

```python

**Purpose**: Maintain consistent person IDs across frames for timeline analysis.

# Run detection with confidence threshold

---

results = model.predict(frame, conf=0.45, iou=0.5)**Purpose**: Normalize video input for consistent ML processing across different camera types.**Purpose**: Normalize video input for consistent ML processing across different camera types.

## Pose Estimation (MediaPipe)



```mermaid

flowchart TD# Extract detections

    A[Person Crop] --> B[MediaPipe Pose]

    B --> C[33 Keypoints]for box in results[0].boxes:

    C --> D[Joint Angles]

    C --> E[Body Position]    detection = {**Input Types**:**Input Types**:

    C --> F[Movement Analysis]

            'bbox': [x1, y1, x2, y2],

    D --> G[Shoulder Angle]

    D --> H[Hip Angle]        'class_name': model.names[int(box.cls[0])],- Standard MP4/MOV walkthrough videos (phone/CCTV)- Standard MP4/MOV walkthrough videos (phone/CCTV)

    D --> I[Knee Angle]

            'confidence': float(box.conf[0]),

    E --> J[Standing/Sitting]

    E --> K[Bent Over]        'timestamp': frame_timestamp- 360° factory surveillance videos (equirectangular format)- 360° factory surveillance videos (equirectangular format)

    E --> L[Arms Overhead]

        }

    F --> M[Walking Speed]

    F --> N[Lifting Motion]```

    F --> O[Repetitive Actions]

    

    style A fill:#e3f2fd

    style B fill:#e8f5e8---```python```python

    style C fill:#e3f2fd

    style D fill:#e8f5e8

    style E fill:#e3f2fd

    style F fill:#e8f5e8## 3. Tracking Layer (DeepSORT)# Load and sample video frames at 2 FPS# Load and sample video frames at 2 FPS

    style G fill:#e3f2fd

    style H fill:#e8f5e8

    style I fill:#e3f2fd

    style J fill:#e8f5e8**Purpose**: Maintain consistent person IDs across frames for timeline analysis.cap = cv2.VideoCapture(video_path)cap = cv2.VideoCapture(video_path)

    style K fill:#e3f2fd

    style L fill:#e8f5e8

    style M fill:#e3f2fd

    style N fill:#e8f5e8```pythonsample_interval = int(fps / 2)  # Sample every 0.5 secondssample_interval = int(fps / 2)  # Sample every 0.5 seconds

    style O fill:#e3f2fd

```# Update tracker with new detections



**Purpose**: Analyze worker posture and movements for safety violations.tracks = tracker.update(detections)



---



## Rule Engine (Industry-Specific)# Build person timelinesfor frame_idx in range(0, total_frames, sample_interval):for frame_idx in range(0, total_frames, sample_interval):



```mermaidfor track in tracks:

flowchart TD

    A[Person Timeline] --> B[Rule Evaluation]    person_timeline[track.track_id].append({    ret, frame = cap.read()    ret, frame = cap.read()

    B --> C[PPE Rules]

    B --> D[Safety Zone Rules]        'bbox': track.bbox,

    B --> E[Posture Rules]

    B --> F[Equipment Rules]        'timestamp': timestamp,    if ret:    if ret:

    

    C --> G[Helmet Required]        'detections': associated_ppe  # PPE items near this person

    C --> H[Vest Required]

    C --> I[Gloves Required]    })        frame = cv2.resize(frame, (640, 640))  # YOLOv8 input size        frame = cv2.resize(frame, (640, 640))  # YOLOv8 input size

    

    D --> J[Restricted Areas]```

    D --> K[Emergency Exits]

    D --> L[Hazard Zones]        frame = frame / 255.0  # Normalize to [0,1]        frame = frame / 255.0  # Normalize to [0,1]

    

    E --> M[Safe Lifting]---

    E --> N[Ergonomic Position]

    E --> O[No Overhead Work]        frames.append((frame_idx, timestamp, frame))        frames.append((frame_idx, timestamp, frame))

    

    F --> P[Machinery Safety]## 4. Pose Estimation (MediaPipe)

    F --> Q[Tool Usage]

    F --> R[Emergency Equipment]``````

    

    G --> S[Violation Detection]**Purpose**: Analyze worker posture and movements for safety violations.

    H --> S

    I --> S

    J --> S

    K --> S```python

    L --> S

    M --> S# Extract pose keypoints for each detected person------

    N --> S

    O --> Spose_results = pose_model.process(person_crop)

    P --> S

    Q --> S

    R --> S

    # Analyze key angles and positions

    style A fill:#e3f2fd

    style B fill:#e8f5e8shoulder_angle = calculate_angle(keypoints['left_shoulder'], ## 2. Object Detection Layer (YOLOv8)## 2. Object Detection Layer (YOLOv8)

    style C fill:#e3f2fd

    style D fill:#e8f5e8                               keypoints['left_elbow'], 

    style E fill:#e3f2fd

    style F fill:#e8f5e8                               keypoints['left_wrist'])

    style S fill:#e3f2fd

```



**Purpose**: Apply business logic to detect compliance violations.# Detect unsafe postures**Purpose**: Identify people, PPE, equipment, hazards, and safety infrastructure.**Purpose**: Identify people, PPE, equipment, hazards, and safety infrastructure.



---if shoulder_angle > 90:  # Arms raised overhead



## Evidence Generation & Scoring    violations.append('unsafe_lifting_posture')



```mermaid```

flowchart TD

    A[Violation Detected] --> B[Extract Video Clip]**Custom Detection Classes**:**Custom Detection Classes**:

    B --> C[2 sec before + 3 sec after]

    C --> D[Generate Thumbnail]---

    D --> E[Encode H.264/avc1]

    E --> F[Save Evidence Files]```yaml```yaml

    

    F --> G[Evidence Database]## 5. Rule Engine (Industry-Specific)

    G --> H[Clip Path]

    G --> I[Thumbnail Path]names:names:

    G --> J[Violation Metadata]

    **Purpose**: Apply business logic to detect compliance violations.

    A --> K[Calculate Compliance Score]

    K --> L[Total Person Minutes]  0: person  0: person

    K --> M[Violation Minutes]

    L --> N[Score = 100 - (violations/total * 100)]```python

    M --> N

    # Manufacturing safety rules  1: helmet  1: helmet

    N --> O[Grade Assignment]

    O --> P[A: 90-100%]rules = {

    O --> Q[B: 80-89%]

    O --> R[C: 70-79%]    'helmet_required': lambda person: has_ppe(person, 'helmet'),  2: vest    2: vest  

    O --> S[F: <70%]

        'vest_required': lambda person: has_ppe(person, 'vest'),

    style A fill:#e3f2fd

    style B fill:#e8f5e8    'no_phones': lambda person: not has_item(person, 'phone'),  3: gloves  3: gloves

    style C fill:#e3f2fd

    style D fill:#e8f5e8    'proper_lifting': lambda person: check_lifting_posture(person)

    style E fill:#e3f2fd

    style F fill:#e8f5e8}  4: mask  4: mask

    style G fill:#e3f2fd

    style H fill:#e8f5e8

    style I fill:#e3f2fd

    style J fill:#e8f5e8# Evaluate each person against rules  5: exit_sign  5: exit_sign

    style K fill:#e3f2fd

    style L fill:#e8f5e8for person_id, timeline in person_timelines.items():

    style M fill:#e3f2fd

    style N fill:#e8f5e8    for frame_data in timeline:  6: fire_extinguisher  6: fire_extinguisher

    style O fill:#e3f2fd

    style P fill:#e8f5e8        for rule_name, rule_func in rules.items():

    style Q fill:#e3f2fd

    style R fill:#e8f5e8            if not rule_func(frame_data):  7: spill  7: spill

    style S fill:#e3f2fd

```                violations.append({



**Purpose**: Generate visual evidence and calculate compliance score.                    'person_id': person_id,  8: obstruction  8: obstruction



---                    'rule': rule_name,



## Deployment Architecture                    'timestamp': frame_data['timestamp'],  9: machinery  9: machinery



```mermaid                    'bbox': frame_data['bbox']

flowchart TD

    A[Video Upload] --> B[FastAPI Server]                })``````

    B --> C[ML Pipeline Queue]

    C --> D[Video Preprocessing]```

    D --> E[YOLOv8 Detection]

    E --> F[DeepSORT Tracking]

    F --> G[MediaPipe Pose]

    G --> H[Rule Engine]---

    H --> I[Evidence Generation]

    I --> J[Results Storage]```python```python

    J --> K[Web Dashboard]

    ## 6. Evidence Generation & Scoring

    L[Static File Server] --> M[Video Evidence]

    L --> N[Thumbnails]# Run detection with confidence threshold# Run detection with confidence threshold

    L --> O[Report PDFs]

    **Purpose**: Generate visual evidence and calculate compliance score.

    K --> P[Compliance Score]

    K --> Q[Violation List]results = model.predict(frame, conf=0.45, iou=0.5)results = model.predict(frame, conf=0.45, iou=0.5)

    K --> R[Evidence Viewer]

    ```python

    style A fill:#e3f2fd

    style B fill:#e8f5e8# Generate evidence clips for violations

    style C fill:#e3f2fd

    style D fill:#e8f5e8def generate_evidence(violation, video_path):

    style E fill:#e3f2fd

    style F fill:#e8f5e8    start_time = violation['timestamp'] - 2  # 2 sec before# Extract detections# Extract detections

    style G fill:#e3f2fd

    style H fill:#e8f5e8    end_time = violation['timestamp'] + 3    # 3 sec after

    style I fill:#e3f2fd

    style J fill:#e8f5e8    for box in results[0].boxes:for box in results[0].boxes:

    style K fill:#e3f2fd

    style L fill:#e8f5e8    # Extract video clip

    style M fill:#e3f2fd

    style N fill:#e8f5e8    clip = extract_video_segment(video_path, start_time, end_time)    detection = {    detection = {

    style O fill:#e3f2fd

    style P fill:#e8f5e8    

    style Q fill:#e3f2fd

    style R fill:#e8f5e8    # Generate thumbnail        'bbox': [x1, y1, x2, y2],        'bbox': [x1, y1, x2, y2],

```

    thumbnail = extract_frame(video_path, violation['timestamp'])

---

            'class_name': model.names[int(box.cls[0])],        'class_name': model.names[int(box.cls[0])],

## Output Format

    return {

```json

{        'clip_path': save_clip(clip),        'confidence': float(box.conf[0]),        'confidence': float(box.conf[0]),

  "analysis_id": "uuid",

  "video_info": {        'thumbnail_path': save_thumbnail(thumbnail),

    "filename": "factory_tour.mp4",

    "duration": 120.5,        'description': violation['rule']        'timestamp': frame_timestamp        'timestamp': frame_timestamp

    "total_frames": 3612

  },    }

  "compliance_score": {

    "score": 78.5,    }    }

    "grade": "B",

    "total_violations": 12# Calculate compliance score

  },

  "violations": [total_person_minutes = sum(timeline_duration for timeline in person_timelines.values())``````

    {

      "id": "V-001",violation_minutes = sum(violation_duration for violation in violations)

      "type": "missing_helmet",

      "person_id": "person_3",compliance_score = max(0, 100 - (violation_minutes / total_person_minutes * 100))

      "timestamp": 45.2,

      "confidence": 0.89,```

      "evidence": {

        "thumbnail": "/outputs/thumb_V-001.jpg",------

        "clip": "/outputs/clip_V-001.mp4"

      }---

    }

  ],

  "summary": {

    "people_detected": 8,## Output Format

    "avg_compliance_time": 0.785,

    "most_common_violation": "missing_helmet"## 3. Tracking Layer (DeepSORT)## 3. Tracking Layer (DeepSORT)

  }

}```json

```

{

---

  "analysis_id": "uuid",

## Technical Implementation

  "video_info": {**Purpose**: Maintain consistent person IDs across frames for timeline analysis.**Purpose**: Maintain consistent person IDs across frames for timeline analysis.

### Model Requirements

- **YOLOv8**: Object detection (persons, PPE, hazards)    "filename": "factory_tour.mp4",

- **DeepSORT**: Multi-object tracking across frames  

- **MediaPipe**: Human pose estimation    "duration": 120.5,

- **OpenCV**: Video processing and preprocessing

    "total_frames": 3612

### Performance Specifications

- **Processing Speed**: 2 FPS real-time analysis  },```python```python

- **Input Resolution**: 640x640 (optimized for inference)

- **Detection Accuracy**: >90% for persons, >85% for PPE  "compliance_score": {

- **Supported Formats**: MP4, MOV, AVI (standard + 360°)

    "score": 78.5,# Update tracker with new detections# Update tracker with new detections

### Industry Rule Sets

- **Manufacturing**: Helmet, vest, gloves, safety zones    "grade": "B",

- **Construction**: Hard hat, harness, restricted areas  

- **Food Processing**: Hairnet, gloves, apron, hygiene    "total_violations": 12tracks = tracker.update(detections)tracks = tracker.update(detections)

- **Healthcare**: Mask, gloves, proper PPE protocols

  },

### API Integration

```python  "violations": [

# Upload video for analysis

POST /upload    {

{

  "video": file,      "id": "V-001",# Build person timelines# Build person timelines

  "industry_pack": "manufacturing"

}      "type": "missing_helmet",



# Get analysis results        "person_id": "person_3",for track in tracks:for track in tracks:

GET /analysis/{analysis_id}

{      "timestamp": 45.2,

  "status": "complete",

  "compliance_score": 78.5,      "confidence": 0.89,    person_timeline[track.track_id].append({    person_timeline[track.track_id].append({

  "violations": [...],

  "evidence": [...]      "evidence": {

}

```        "thumbnail": "/outputs/thumb_V-001.jpg",        'bbox': track.bbox,        'bbox': track.bbox,

        "clip": "/outputs/clip_V-001.mp4"

      }        'timestamp': timestamp,        'timestamp': timestamp,

    }

  ],        'detections': associated_ppe  # PPE items near this person        'detections': associated_ppe  # PPE items near this person

  "summary": {

    "people_detected": 8,    })    })

    "avg_compliance_time": 0.785,

    "most_common_violation": "missing_helmet"``````

  }

}

```

------

---



## Technical Implementation

## 4. Pose Estimation (MediaPipe)## 4. Pose Estimation (MediaPipe)

### Model Requirements

- **YOLOv8**: Object detection (persons, PPE, hazards)

- **DeepSORT**: Multi-object tracking across frames  

- **MediaPipe**: Human pose estimation**Purpose**: Analyze worker posture and movements for safety violations.**Purpose**: Analyze worker posture and movements for safety violations.

- **OpenCV**: Video processing and preprocessing



### Performance Specifications

- **Processing Speed**: 2 FPS real-time analysis```python```python

- **Input Resolution**: 640x640 (optimized for inference)

- **Detection Accuracy**: >90% for persons, >85% for PPE# Extract pose keypoints for each detected person# Extract pose keypoints for each detected person

- **Supported Formats**: MP4, MOV, AVI (standard + 360°)

pose_results = pose_model.process(person_crop)pose_results = pose_model.process(person_crop)

### Industry Rule Sets

- **Manufacturing**: Helmet, vest, gloves, safety zones

- **Construction**: Hard hat, harness, restricted areas  

- **Food Processing**: Hairnet, gloves, apron, hygiene# Analyze key angles and positions# Analyze key angles and positions

- **Healthcare**: Mask, gloves, proper PPE protocols

shoulder_angle = calculate_angle(keypoints['left_shoulder'], shoulder_angle = calculate_angle(keypoints['left_shoulder'], 

### Deployment Architecture

```mermaid                               keypoints['left_elbow'],                                keypoints['left_elbow'], 

flowchart LR

    A[Video Upload] --> B[FastAPI Server]                               keypoints['left_wrist'])                               keypoints['left_wrist'])

    B --> C[ML Pipeline]

    C --> D[Evidence Generation]

    D --> E[Results Dashboard]

    # Detect unsafe postures# Detect unsafe postures

    style A fill:#e3f2fd

    style B fill:#e8f5e8if shoulder_angle > 90:  # Arms raised overheadif shoulder_angle > 90:  # Arms raised overhead

    style C fill:#e3f2fd

    style D fill:#e8f5e8    violations.append('unsafe_lifting_posture')    violations.append('unsafe_lifting_posture')

    style E fill:#e3f2fd

`````````



### API Integration

```python

# Upload video for analysis------

POST /upload

{

  "video": file,

  "industry_pack": "manufacturing"## 5. Rule Engine (Industry-Specific)## 5. Rule Engine (Industry-Specific)

}



# Get analysis results  

GET /analysis/{analysis_id}**Purpose**: Apply business logic to detect compliance violations.**Purpose**: Apply business logic to detect compliance violations.

{

  "status": "complete",

  "compliance_score": 78.5,

  "violations": [...],```python```python

  "evidence": [...]

}# Manufacturing safety rules# Manufacturing safety rules

```
rules = {rules = {

    'helmet_required': lambda person: has_ppe(person, 'helmet'),    'helmet_required': lambda person: has_ppe(person, 'helmet'),

    'vest_required': lambda person: has_ppe(person, 'vest'),    'vest_required': lambda person: has_ppe(person, 'vest'),

    'no_phones': lambda person: not has_item(person, 'phone'),    'no_phones': lambda person: not has_item(person, 'phone'),

    'proper_lifting': lambda person: check_lifting_posture(person)    'proper_lifting': lambda person: check_lifting_posture(person)

}}



# Evaluate each person against rules# Evaluate each person against rules

for person_id, timeline in person_timelines.items():for person_id, timeline in person_timelines.items():

    for frame_data in timeline:    for frame_data in timeline:

        for rule_name, rule_func in rules.items():        for rule_name, rule_func in rules.items():

            if not rule_func(frame_data):            if not rule_func(frame_data):

                violations.append({                violations.append({

                    'person_id': person_id,                    'person_id': person_id,

                    'rule': rule_name,                    'rule': rule_name,

                    'timestamp': frame_data['timestamp'],                    'timestamp': frame_data['timestamp'],

                    'bbox': frame_data['bbox']                    'bbox': frame_data['bbox']

                })                })

``````



------



## 6. Evidence Generation & Scoring## 6. Evidence Generation & Scoring



**Purpose**: Generate visual evidence and calculate compliance score.**Purpose**: Generate visual evidence and calculate compliance score.



```python```python

# Generate evidence clips for violations# Generate evidence clips for violations

def generate_evidence(violation, video_path):def generate_evidence(violation, video_path):

    start_time = violation['timestamp'] - 2  # 2 sec before    start_time = violation['timestamp'] - 2  # 2 sec before

    end_time = violation['timestamp'] + 3    # 3 sec after    end_time = violation['timestamp'] + 3    # 3 sec after

        

    # Extract video clip    # Extract video clip

    clip = extract_video_segment(video_path, start_time, end_time)    clip = extract_video_segment(video_path, start_time, end_time)

        

    # Generate thumbnail    # Generate thumbnail

    thumbnail = extract_frame(video_path, violation['timestamp'])    thumbnail = extract_frame(video_path, violation['timestamp'])

        

    return {    return {

        'clip_path': save_clip(clip),        'clip_path': save_clip(clip),

        'thumbnail_path': save_thumbnail(thumbnail),        'thumbnail_path': save_thumbnail(thumbnail),

        'description': violation['rule']        'description': violation['rule']

    }    }



# Calculate compliance score# Calculate compliance score

total_person_minutes = sum(timeline_duration for timeline in person_timelines.values())total_person_minutes = sum(timeline_duration for timeline in person_timelines.values())

violation_minutes = sum(violation_duration for violation in violations)violation_minutes = sum(violation_duration for violation in violations)

compliance_score = max(0, 100 - (violation_minutes / total_person_minutes * 100))compliance_score = max(0, 100 - (violation_minutes / total_person_minutes * 100))

``````



------



## Output Format## Output Format



```json```json

{{

  "analysis_id": "uuid",  "analysis_id": "uuid",

  "video_info": {  "video_info": {

    "filename": "factory_tour.mp4",    "filename": "factory_tour.mp4",

    "duration": 120.5,    "duration": 120.5,

    "total_frames": 3612    "total_frames": 3612

  },  },

  "compliance_score": {  "compliance_score": {

    "score": 78.5,    "score": 78.5,

    "grade": "B",    "grade": "B",

    "total_violations": 12    "total_violations": 12

  },  },

  "violations": [  "violations": [

    {    {

      "id": "V-001",      "id": "V-001",

      "type": "missing_helmet",      "type": "missing_helmet",

      "person_id": "person_3",      "person_id": "person_3",

      "timestamp": 45.2,      "timestamp": 45.2,

      "confidence": 0.89,      "confidence": 0.89,

      "evidence": {      "evidence": {

        "thumbnail": "/outputs/thumb_V-001.jpg",        "thumbnail": "/outputs/thumb_V-001.jpg",

        "clip": "/outputs/clip_V-001.mp4"        "clip": "/outputs/clip_V-001.mp4"

      }      }

    }    }

  ],  ],

  "summary": {  "summary": {

    "people_detected": 8,    "people_detected": 8,

    "avg_compliance_time": 0.785,    "avg_compliance_time": 0.785,

    "most_common_violation": "missing_helmet"    "most_common_violation": "missing_helmet"

  }  }

}}

``````



---```mermaid

flowchart TD

## Technical Implementation    A[🖼️ Input Frame<br/>640x640x3] --> B[🧠 YOLOv8 Model<br/>Backbone + Head]

    B --> C[📦 Raw Detections<br/>Boxes + Classes + Conf]

### Model Requirements    C --> D{Confidence > 0.45?}

- **YOLOv8**: Object detection (persons, PPE, hazards)    D -->|Yes| E[✅ Valid Detection]

- **DeepSORT**: Multi-object tracking across frames      D -->|No| F[❌ Filter Out]

- **MediaPipe**: Human pose estimation    E --> G[📋 Detection List<br/>person, helmet, vest, etc.]

- **OpenCV**: Video processing and preprocessing    

    H[🏷️ Custom Classes] --> B

### Performance Specifications    I[person<br/>helmet<br/>vest<br/>gloves<br/>mask<br/>exit_sign<br/>spill] --> H

- **Processing Speed**: 2 FPS real-time analysis    

- **Input Resolution**: 640x640 (optimized for inference)    style A fill:#e3f2fd

- **Detection Accuracy**: >90% for persons, >85% for PPE    style B fill:#fff3e0

- **Supported Formats**: MP4, MOV, AVI (standard + 360°)    style G fill:#e8f5e8

    style I fill:#f3e5f5

### Industry Rule Sets```

- **Manufacturing**: Helmet, vest, gloves, safety zones

- **Construction**: Hard hat, harness, restricted areas  **Detection Output Format:**

- **Food Processing**: Hairnet, gloves, apron, hygiene```mermaid

- **Healthcare**: Mask, gloves, proper PPE protocolsflowchart LR

    A[🎯 Detection] --> B[📍 Bounding Box<br/>x1, y1, x2, y2]

### Deployment Architecture    A --> C[🏷️ Class Name<br/>person, helmet, etc.]

```mermaid    A --> D[📊 Confidence<br/>0.0 - 1.0]

flowchart LR    A --> E[⏰ Timestamp<br/>Frame time]

    A[Video Upload] --> B[FastAPI Server]    

    B --> C[ML Pipeline]    style A fill:#ffebee

    C --> D[Evidence Generation]    style B fill:#e8f5e8

    D --> E[Results Dashboard]    style C fill:#fff3e0

        style D fill:#f3e5f5

    style A fill:#e3f2fd    style E fill:#e1f5fe

    style B fill:#e8f5e8```

    style C fill:#e3f2fd

    style D fill:#e8f5e8**Purpose**: Identify people, PPE, equipment, hazards, and safety infrastructure in factory environments.

    style E fill:#e3f2fd

```**Model Architecture**:

```python

### API Integration# Load base YOLOv8 model

```pythonmodel = YOLO('yolov8n.pt')  # or yolov8s/m/l/x for better accuracy

# Upload video for analysis

POST /upload# Fine-tune on factory dataset (required for production)

{model.train(

  "video": file,    data='factory_compliance.yaml',  # Custom dataset

  "industry_pack": "manufacturing"    epochs=100,

}    imgsz=640,

    batch=16

# Get analysis results  )

GET /analysis/{analysis_id}```

{

  "status": "complete",**Custom Detection Classes**:

  "compliance_score": 78.5,```yaml

  "violations": [...],# factory_compliance.yaml

  "evidence": [...]names:

}  0: person

```  1: helmet          # Safety helmets/hard hats
  2: vest            # High-vis safety vests  
  3: gloves          # Work gloves
  4: mask            # Face masks/respirators
  5: hairnet         # Food industry hairnets
  6: exit_sign       # Emergency exit signs
  7: fire_extinguisher
  8: spill           # Liquid spills/hazards
  9: obstruction     # Blocked walkways
  10: machinery      # Industrial equipment
```

**Inference Process**:
```python
# Run detection on each frame
results = model.predict(frame, conf=0.45, iou=0.5)

# Extract detections
detections = []
for box in results[0].boxes:
    detection = {
        'bbox': [x1, y1, x2, y2],  # Bounding box coordinates
        'class_id': int(box.cls[0]),
        'class_name': model.names[int(box.cls[0])],
        'confidence': float(box.conf[0]),
        'timestamp': frame_timestamp
    }
    detections.append(detection)
```

**Output Format**: List of detections per frame with bbox, class, confidence

---

## 3️⃣ Tracking Layer (DeepSORT/ByteTrack)

```mermaid
flowchart TD
    A[🎯 Frame 1<br/>Detections] --> D[🔗 DeepSORT<br/>Tracker]
    B[🎯 Frame 2<br/>Detections] --> D
    C[🎯 Frame N<br/>Detections] --> D
    
    D --> E[📊 Track Matching<br/>IoU + Appearance]
    E --> F[🆔 Assign Track IDs<br/>person_1, person_2]
    F --> G[⏱️ Build Timeline<br/>Track History]
    
    G --> H[👤 Person 1<br/>Frames 1-25]
    G --> I[👤 Person 2<br/>Frames 5-30]
    G --> J[👤 Person 3<br/>Frames 15-40]
    
    style D fill:#fff3e0
    style E fill:#f3e5f5
    style F fill:#e8f5e8
    style G fill:#ffebee
```

**Track Timeline Structure:**
```mermaid
flowchart LR
    A[👤 Track ID: 007] --> B[📍 Position History<br/>Frame by frame]
    A --> C[🏷️ Class Consistency<br/>Always person]
    A --> D[⏰ Duration<br/>First to last frame]
    A --> E[📊 Confidence Trend<br/>Detection quality]
    
    style A fill:#e1f5fe
    style B fill:#e8f5e8
    style C fill:#fff3e0
    style D fill:#f3e5f5
    style E fill:#ffebee
```

**Purpose**: Track workers and equipment across frames to enable temporal compliance analysis.

**Why Tracking Matters**: Compliance violations often occur over time (e.g., "worker without helmet for 30+ seconds"), not just single frames.

**Implementation**:
```python
from deep_sort_realtime import DeepSort

# Initialize tracker
tracker = DeepSort(max_age=30, n_init=3, max_iou_distance=0.7)

# Track detections across frames
tracks = tracker.update_tracks(detections, frame=current_frame)

# Extract track information
tracked_objects = []
for track in tracks:
    if track.is_confirmed():
        tracked_objects.append({
            'track_id': track.track_id,
            'bbox': track.to_tlwh(),  # [x, y, width, height]
            'class_name': track.get_det_class(),
            'confidence': track.get_det_conf(),
            'track_age': track.age,  # Frames since first detection
            'timestamp': frame_timestamp
        })
```

**Track History Management**:
```python
# Build timeline for each tracked person
person_timelines = {}
for detection in tracked_objects:
    if detection['class_name'] == 'person':
        track_id = detection['track_id']
        if track_id not in person_timelines:
            person_timelines[track_id] = []
        person_timelines[track_id].append(detection)
```

**Output**: Timeline of detections for each tracked person/object with persistent IDs

---

## 4️⃣ Pose Estimation Layer (MediaPipe/HRNet)

```mermaid
flowchart TD
    A[👤 Person Detection<br/>from YOLO] --> B[✂️ Crop Person<br/>from Frame]
    B --> C[🤸 MediaPipe Pose<br/>33 Keypoints]
    C --> D[📊 Landmark Analysis<br/>Body joints]
    D --> E[📐 Calculate Angles<br/>Spine, Arms, Legs]
    E --> F{Risk Analysis}
    F -->|Bending > 30°| G[⚠️ Excessive Bending]
    F -->|Arms > 60°| H[⚠️ Overhead Reaching]
    F -->|Normal Range| I[✅ Safe Posture]
    
    G --> J[📋 Risk Report]
    H --> J
    I --> J
    
    style A fill:#e3f2fd
    style C fill:#fff3e0
    style E fill:#f3e5f5
    style G fill:#ffebee
    style H fill:#ffebee
    style I fill:#e8f5e8
```

**Pose Keypoints Map:**
```mermaid
flowchart LR
    A[🧠 Head Points<br/>0-10] --> E[📊 33 Keypoints]
    B[🤲 Arm Points<br/>11-16] --> E
    C[🦵 Leg Points<br/>23-28] --> E
    D[🫁 Torso Points<br/>11-12, 23-24] --> E
    
    E --> F[📐 Angle Analysis]
    F --> G[⚠️ Risk Assessment]
    
    style E fill:#e1f5fe
    style F fill:#fff3e0
    style G fill:#ffebee
```

**Purpose**: Analyze worker body posture for ergonomic compliance and safety violations.

**Model Pipeline**:
```python
import mediapipe as mp

# Initialize MediaPipe Pose
mp_pose = mp.solutions.pose
pose_estimator = mp_pose.Pose(
    static_image_mode=False,
    model_complexity=1,  # Balance speed vs accuracy
    enable_segmentation=False,
    min_detection_confidence=0.5
)

# Process person crops from YOLO detections
def analyze_posture(person_bbox, frame):
    # Crop person from frame using YOLO bbox
    x1, y1, x2, y2 = person_bbox
    person_crop = frame[y1:y2, x1:x2]
    
    # Run pose estimation
    results = pose_estimator.process(person_crop)
    
    if results.pose_landmarks:
        # Extract 33 keypoints
        landmarks = []
        for landmark in results.pose_landmarks.landmark:
            landmarks.append([landmark.x, landmark.y, landmark.z])
        
        # Analyze ergonomic risks
        risks = analyze_ergonomic_risks(landmarks)
        return {
            'keypoints': landmarks,
            'risks': risks,
            'confidence': results.pose_landmarks.visibility
        }
    return None
```

**Ergonomic Risk Analysis**:
```python
def analyze_ergonomic_risks(landmarks):
    risks = []
    
    # Check bending posture (spine angle)
    shoulder_to_hip_angle = calculate_angle(
        landmarks[11],  # Left shoulder
        landmarks[23],  # Left hip
        landmarks[25]   # Left knee
    )
    
    if shoulder_to_hip_angle < 120:  # Degrees
        risks.append({
            'type': 'excessive_bending',
            'severity': 'major',
            'description': 'Worker bending beyond safe limits'
        })
    
    # Check arm reaching (shoulder elevation)
    arm_elevation = calculate_arm_elevation(landmarks)
    if arm_elevation > 60:  # Degrees above shoulder
        risks.append({
            'type': 'overhead_reaching',
            'severity': 'minor',
            'description': 'Extended overhead reaching'
        })
    
    return risks
```

**Output**: Pose keypoints + ergonomic risk flags per tracked person

---

## 5️⃣ Rule Engine Layer (Industry-Specific)

```mermaid
flowchart TD
    A[🎯 Detections] --> D[⚖️ Rule Engine]
    B[🏃 Tracking Data] --> D
    C[🤸 Pose Data] --> D
    
    D --> E{Industry Pack}
    E -->|Manufacturing| F[🏭 PPE Rules<br/>Helmet + Vest]
    E -->|Food| G[🍕 Hygiene Rules<br/>Hairnet + Gloves]
    E -->|Chemical| H[⚗️ Safety Rules<br/>Respirator + Suit]
    
    F --> I[📋 Rule Evaluation]
    G --> I
    H --> I
    
    I --> J{Violation Found?}
    J -->|Yes| K[⏰ Check Duration<br/>> 5 seconds?]
    J -->|No| L[✅ Compliant]
    
    K -->|Yes| M[🚨 Log Violation]
    K -->|No| N[⏳ Continue Monitoring]
    
    style D fill:#fff3e0
    style E fill:#f3e5f5
    style I fill:#e8f5e8
    style M fill:#ffebee
    style L fill:#e8f5e8
```

**Rule Evaluation Logic:**
```mermaid
flowchart LR
    A[👤 Person Detected] --> B{Has Helmet?}
    B -->|No| C[⏰ Start Timer]
    B -->|Yes| D[✅ Compliant]
    
    C --> E{Timer > 5s?}
    E -->|Yes| F[🚨 Violation:<br/>Missing Helmet]
    E -->|No| G[⏳ Keep Watching]
    
    F --> H[📎 Generate Evidence]
    
    style A fill:#e3f2fd
    style B fill:#fff3e0
    style C fill:#f3e5f5
    style F fill:#ffebee
    style D fill:#e8f5e8
```

**Purpose**: Apply configurable compliance rules based on detections, tracking, and pose data.

**Rule Structure**:
```python
class ComplianceRule:
    def __init__(self, rule_id, industry_pack, severity):
        self.rule_id = rule_id
        self.industry_pack = industry_pack  # 'food', 'manufacturing', etc.
        self.severity = severity  # 'critical', 'major', 'minor'
        self.persistence_threshold = 5  # Seconds
        
    def evaluate(self, detections, tracks, pose_data, timestamp):
        # Rule-specific logic
        pass

# Example: PPE compliance rule
class HelmetComplianceRule(ComplianceRule):
    def evaluate(self, detections, tracks, pose_data, timestamp):
        violations = []
        
        # Find all tracked persons
        persons = [d for d in detections if d['class_name'] == 'person']
        helmets = [d for d in detections if d['class_name'] == 'helmet']
        
        for person in persons:
            # Check if person has nearby helmet detection
            has_helmet = self.check_helmet_proximity(person, helmets)
            
            if not has_helmet:
                # Check temporal persistence
                track_id = person.get('track_id')
                violation_duration = self.check_violation_duration(
                    track_id, 'missing_helmet', timestamp
                )
                
                if violation_duration > self.persistence_threshold:
                    violations.append({
                        'rule_id': 'PPE.helmet.required',
                        'type': 'missing_helmet',
                        'severity': 'major',
                        'track_id': track_id,
                        'bbox': person['bbox'],
                        'timestamp': timestamp,
                        'duration': violation_duration,
                        'confidence': 0.9
                    })
        
        return violations
```

**Industry Pack Rules**:
```python
# Manufacturing industry pack
manufacturing_rules = [
    HelmetComplianceRule(),
    VestComplianceRule(),
    ExitBlockageRule(),
    PostureComplianceRule()
]

# Food processing industry pack  
food_rules = [
    HairnetComplianceRule(),
    GloveComplianceRule(),
    HygieneZoneRule(),
    TemperatureMonitoringRule()
]
```

**Output**: List of compliance violations with severity, evidence, and temporal context

---

## 6️⃣ Temporal Reasoning Layer (LSTM/Transformer Head)

```mermaid
flowchart TD
    A[📊 Track Timeline<br/>Person over time] --> B[🔢 Feature Extraction<br/>Position, PPE, Pose]
    B --> C[📈 Sequence Features<br/>Time series data]
    C --> D{Implementation}
    
    D -->|Current| E[📋 Rule-based<br/>Persistence Logic]
    D -->|Planned| F[🧠 LSTM Model<br/>Sequence Learning]
    
    E --> G[⏰ Duration Check<br/>Violation > 5s?]
    F --> H[🎯 ML Prediction<br/>Violation probability]
    
    G --> I[📊 Simple Logic<br/>If-then rules]
    H --> J[🤖 Smart Patterns<br/>Complex sequences]
    
    I --> K[📋 Current Output]
    J --> L[🚀 Future Output]
    
    style E fill:#e8f5e8
    style F fill:#fff3e0
    style G fill:#f3e5f5
    style H fill:#ffebee
    style K fill:#e1f5fe
    style L fill:#e1f5fe
```

**Current vs Planned Temporal Logic:**
```mermaid
flowchart LR
    A[📊 Current:<br/>Rule-based] --> B[✅ Simple persistence<br/>✅ Duration thresholds<br/>✅ Basic patterns]
    
    C[🚀 Planned:<br/>ML-based] --> D[🎯 Complex sequences<br/>🎯 Pattern learning<br/>🎯 Contextual analysis]
    
    style A fill:#e8f5e8
    style B fill:#e8f5e8
    style C fill:#fff3e0
    style D fill:#fff3e0
```

**Purpose**: Learn complex violation patterns that occur over time sequences.

**⚠️ Current Status**: Basic rule-based temporal logic implemented; ML temporal model planned.

**Planned Architecture**:
```python
import torch
import torch.nn as nn

class ComplianceTemporalModel(nn.Module):
    def __init__(self, input_dim=128, hidden_dim=256, num_classes=10):
        super().__init__()
        self.lstm = nn.LSTM(input_dim, hidden_dim, batch_first=True)
        self.classifier = nn.Linear(hidden_dim, num_classes)
        
    def forward(self, sequence_features):
        # sequence_features: [batch, sequence_length, feature_dim]
        lstm_out, _ = self.lstm(sequence_features)
        predictions = self.classifier(lstm_out[:, -1, :])  # Last timestep
        return predictions

# Feature extraction from detections/tracks
def extract_sequence_features(track_timeline):
    features = []
    for detection in track_timeline:
        feature_vector = [
            detection['confidence'],
            *detection['bbox'],  # Bounding box coordinates
            detection['class_id'],
            detection.get('has_helmet', 0),  # Binary PPE flags
            detection.get('has_vest', 0),
            detection.get('pose_risk_score', 0)
        ]
        features.append(feature_vector)
    return torch.tensor(features)
```

**Current Implementation** (Rule-based):
```python
# Simple temporal persistence checking
def check_violation_persistence(track_id, violation_type, current_time):
    # Check how long violation has been occurring
    violation_history = get_track_violations(track_id, violation_type)
    
    if violation_history:
        start_time = violation_history[0]['timestamp']
        duration = current_time - start_time
        return duration
    return 0
```

**Output**: Enhanced violation predictions with temporal context and confidence

---

## 7️⃣ Evidence Generation & Compliance Scoring

```mermaid
flowchart TD
    A[🚨 Violation Detected] --> B[📎 Evidence Generation]
    B --> C[🎬 Extract Video Clip<br/>±3 seconds]
    B --> D[📸 Capture Screenshot<br/>With bounding boxes]
    
    C --> E[📁 Store Evidence<br/>violation_42.1.mp4]
    D --> F[📁 Store Thumbnail<br/>thumb_42.1.jpg]
    
    E --> G[📊 Compliance Scoring]
    F --> G
    
    G --> H[⚖️ Calculate Penalties]
    H --> I{Severity}
    I -->|Critical| J[-30 Points]
    I -->|Major| K[-20 Points]
    I -->|Minor| L[-10 Points]
    I -->|Warning| M[-3 Points]
    
    J --> N[📈 Final Score<br/>100 - Total Penalties]
    K --> N
    L --> N
    M --> N
    
    N --> O[📋 Grade Assignment<br/>A, B, C, D, F]
    
    style A fill:#ffebee
    style B fill:#fff3e0
    style G fill:#f3e5f5
    style N fill:#e8f5e8
    style O fill:#e1f5fe
```

**Scoring Algorithm Flow:**
```mermaid
flowchart LR
    A[🎯 Base Score: 100] --> B[📊 For each violation]
    B --> C[⚖️ Severity Weight<br/>Critical=30, Major=20]
    C --> D[⏰ Duration Factor<br/>Longer = worse]
    D --> E[📊 Confidence Factor<br/>Higher = more penalty]
    E --> F[➖ Subtract Penalty]
    F --> G[📈 Final Score<br/>Max(0, 100-total)]
    
    style A fill:#e8f5e8
    style C fill:#fff3e0
    style G fill:#e1f5fe
```

**Evidence Extraction**:
```python
import subprocess

def extract_evidence_clip(video_path, timestamp, duration=6):
    # Extract ±3 seconds around violation
    start_time = max(0, timestamp - 3)
    output_path = f"evidence/violation_{timestamp}.mp4"
    
    # Use FFmpeg for precise clip extraction
    cmd = [
        'ffmpeg', '-i', video_path,
        '-ss', str(start_time),
        '-t', str(duration),
        '-c:v', 'libx264',
        '-c:a', 'aac',
        output_path
    ]
    subprocess.run(cmd, capture_output=True)
    return output_path

def generate_thumbnail(frame, detections, violation):
    # Draw bounding boxes on frame
    annotated_frame = frame.copy()
    for detection in detections:
        x1, y1, x2, y2 = detection['bbox']
        cv2.rectangle(annotated_frame, (x1, y1), (x2, y2), (0, 0, 255), 2)
        cv2.putText(annotated_frame, detection['class_name'], 
                   (x1, y1-10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 1)
    
    thumbnail_path = f"evidence/thumb_{violation['timestamp']}.jpg"
    cv2.imwrite(thumbnail_path, annotated_frame)
    return thumbnail_path
```

**Compliance Scoring Algorithm**:
```python
def calculate_compliance_score(violations, video_duration):
    # Severity weights
    SEVERITY_WEIGHTS = {
        'critical': 30,   # -30 points
        'major': 20,      # -20 points  
        'minor': 10,      # -10 points
        'warning': 3      # -3 points
    }
    
    base_score = 100
    penalty = 0
    
    for violation in violations:
        weight = SEVERITY_WEIGHTS[violation['severity']]
        duration_factor = violation.get('duration', 1) / 10  # Normalize duration
        confidence_factor = violation['confidence']
        
        violation_penalty = weight * duration_factor * confidence_factor
        penalty += violation_penalty
    
    final_score = max(0, base_score - penalty)
    
    return {
        'compliance_score': round(final_score, 1),
        'grade': get_letter_grade(final_score),
        'total_violations': len(violations),
        'penalty_breakdown': calculate_penalty_breakdown(violations)
    }
```

**Output Format**:
```json
{
  "compliance_score": 87.5,
  "grade": "B+",
  "violations": [
    {
      "id": "V-001",
      "type": "missing_helmet",
      "severity": "major", 
      "timestamp": 42.1,
      "duration": 8.5,
      "track_id": 7,
      "confidence": 0.91,
      "evidence": {
        "thumbnail": "evidence/thumb_42.1.jpg",
        "clip": "evidence/violation_42.1.mp4"
      }
    }
  ],
  "timeline": [
    {"timestamp": 42.1, "type": "missing_helmet", "severity": "major"}
  ]
}
```

---

## 🔄 Complete Pipeline Flow

```mermaid
flowchart TD
    A[📹 Video Input MP4/360°] --> B{Video Type?}
    B -->|Standard| C[🔧 Resize to 640x640]
    B -->|360°| D[🌐 Convert to Perspective Tiles]
    C --> E[📊 Frame Sampling 2 FPS]
    D --> E
    E --> F[🎯 YOLOv8 Object Detection]
    F --> G[🏃 DeepSORT Tracking]
    G --> H[🤸 MediaPipe Pose Analysis]
    H --> I[⚖️ Rule Engine Evaluation]
    I --> J[⏰ Temporal Reasoning]
    J --> K[📎 Evidence Generation]
    K --> L[📊 Compliance Scoring]
    L --> M[📋 JSON Output Dashboard Ready]
    
    style A fill:#e1f5fe
    style F fill:#fff3e0
    style G fill:#f3e5f5
    style H fill:#e8f5e8
    style I fill:#fff8e1
    style L fill:#ffebee
    style M fill:#e0f2f1
```

## 🏗️ System Architecture Overview

```mermaid
flowchart TB
    subgraph "📥 Input Layer"
        A[Standard Video] 
        B[360° Video]
    end
    
    subgraph "🔧 Preprocessing Layer"
        C[Frame Sampling]
        D[Resize & Normalize]
        E[360° Tile Generation]
    end
    
    subgraph "🧠 AI Model Layer"
        F[YOLOv8 Detection]
        G[DeepSORT Tracking] 
        H[MediaPipe Pose]
    end
    
    subgraph "⚖️ Logic Layer"
        I[Rule Engine]
        J[Temporal Reasoning]
        K[Industry Packs]
    end
    
    subgraph "� Output Layer"
        L[Evidence Generation]
        M[Compliance Scoring]
        N[Dashboard JSON]
    end
    
    A --> C
    B --> E
    C --> D
    E --> D
    D --> F
    F --> G
    G --> H
    H --> I
    I --> J
    J --> L
    L --> M
    M --> N
    K --> I
    
    style A fill:#e3f2fd
    style B fill:#e3f2fd
    style F fill:#fff3e0
    style G fill:#f3e5f5
    style H fill:#e8f5e8
    style I fill:#fff8e1
    style N fill:#e0f2f1
```

## 📊 Data Flow Architecture

```mermaid
flowchart LR
    subgraph "📹 Video"
        A[MP4 File<br/>1920x1080<br/>30 FPS]
    end
    
    subgraph "🔧 Preprocessing" 
        B[Frame Array<br/>640x640x3<br/>2 FPS]
    end
    
    subgraph "🎯 Detection"
        C[Bounding Boxes<br/>[x1,y1,x2,y2]<br/>+ Classes + Conf]
    end
    
    subgraph "🏃 Tracking"
        D[Track Timeline<br/>ID + History<br/>+ Persistence]
    end
    
    subgraph "📋 Violations"
        E[Violation List<br/>Type + Severity<br/>+ Evidence]
    end
    
    subgraph "📊 Output"
        F[JSON Report<br/>Score + Timeline<br/>+ Dashboard Data]
    end
    
    A --> B
    B --> C  
    C --> D
    D --> E
    E --> F
    
    style A fill:#e3f2fd
    style B fill:#fff3e0
    style C fill:#f3e5f5
    style D fill:#e8f5e8
    style E fill:#ffebee
    style F fill:#e0f2f1
```

---

## 🧩 Why This Architecture Works

**✅ Production Ready**: Built on proven SOTA models (YOLOv8, MediaPipe, DeepSORT)  
**🔧 Modular Design**: Each layer can be improved/replaced independently  
**📏 Scalable**: Handles both standard and 360° video formats  
**🏭 Industry Aware**: Configurable rule packs for different sectors  
**📊 Dashboard Compatible**: Structured JSON output plugs directly into Next.js  
**⚡ Efficient**: 2 FPS sampling keeps processing manageable  
**🎯 Accurate**: Multi-layer validation (detection + tracking + temporal + rules)

---

## 🎯 Current Implementation Status

```mermaid
flowchart TD
    subgraph "✅ FULLY IMPLEMENTED (90%)"
        A[📹 Video Preprocessing<br/>OpenCV + 360° support]
        B[🎯 YOLOv8 Detection<br/>Object detection framework]
        C[🏃 DeepSORT Tracking<br/>Person tracking system]
        D[🤸 MediaPipe Pose<br/>Ergonomic analysis]
        E[⚖️ Rule Engine<br/>Industry-specific logic]
        F[📎 Evidence Generation<br/>Clips + screenshots]
        G[📊 Compliance Scoring<br/>Weighted penalty system]
    end
    
    subgraph "⚠️ DEMO/LIMITED"
        H[🏷️ Factory Classes<br/>Using COCO instead]
        I[🎯 Pose Accuracy<br/>Heuristic vs ML-trained]
    end
    
    subgraph "❌ PLANNED"
        J[🧠 LSTM/Transformer<br/>Advanced temporal model]
        K[🎓 Custom Training<br/>Factory-specific dataset]
    end
    
    style A fill:#e8f5e8
    style B fill:#e8f5e8
    style C fill:#e8f5e8
    style D fill:#e8f5e8
    style E fill:#e8f5e8
    style F fill:#e8f5e8
    style G fill:#e8f5e8
    style H fill:#fff3e0
    style I fill:#fff3e0
    style J fill:#ffebee
    style K fill:#ffebee
```

**Production Readiness Assessment:**
```mermaid
pie title Implementation Completeness
    "Fully Implemented" : 70
    "Demo Quality" : 20
    "Missing/Planned" : 10
```

## 2) Component Flow (AI Inference Service)

```mermaid
sequenceDiagram
  participant FE as Frontend (Next.js)
  participant API as AI API (FastAPI)
  participant DET as Detector (YOLO/RT-DETR)
  participant TRK as Tracker (ByteTrack)
  participant POS as Pose (Optional)
  participant RL as Rule Engine
  participant ST as Storage (thumbs/clips)

  FE->>API: POST /analyze { file|url, is360, industry }
  API-->>FE: { jobId }
  loop progress
    FE->>API: GET /status/{jobId}
    API-->>FE: { stage, progress }
  end
  API->>DET: batched frames (tiled if is360)
  DET-->>API: detections per frame
  API->>TRK: detections sequence
  TRK-->>API: tracks with IDs
  API->>POS: (optional) person crops
  POS-->>API: keypoints + posture flags
  API->>RL: tracks + detections + pose
  RL-->>API: violations (type, severity, timestamps)
  API->>ST: write thumbnails & clips
  ST-->>API: evidence URLs
  API-->>FE: GET /results/{jobId} → JSON + evidence links
```

---

## 3) Frontend Feature → How It’s Achieved

- **Upload** (`/video-upload`)
  - Accepts MP4/MOV/AVI and flags 360° videos.
  - Calls `POST /analyze` with `{ is360, industry, file|url }`.

- **Processing** (`/processing`)
  - Polls `GET /status/{jobId}`.
  - Stages reported by backend: `preprocess → detect → track → rules → report`.
  - Progress (0–100) computed server-side; streamed as SSE/WebSocket or simple polling.

- **Results** (`/results`)
  - Fetches `GET /results/{jobId}`.
  - Overlays: Uses `bboxPct: [x,y,w,h]` (percentages) so boxes scale correctly with responsive player.
  - Timeline: Uses `timeline[]` markers with `timestampSec` and `severity`.
  - Thumbnails/Clips: Displays `evidence.thumbUrl` and `evidence.clipUrl` for each violation.

- **Report** (`/report`)
  - Renders `overallScore` + violation breakdown + evidence links.
  - Exports: PDF/CSV generated client-side or requested from API as pre-rendered assets.

- **Industry Packs** (`/industry-pack`)
  - Frontend selects industry; passes value to `/analyze`.
  - Backend activates rule sets accordingly (foundation + specialization of chosen industry).

---

## 4) Minimal, Clear API (Contract)

- **POST** `/analyze`
  - **Request**: `{ videoUrl? string, file? multipart, is360: boolean, industry: 'general'|'pharma'|'food'|'electronics'|'chemicals'|'auto' }`
  - **Response**: `{ jobId: string }`

- **GET** `/status/{jobId}`
  - **Response**: `{ stage: 'preprocess'|'detect'|'track'|'rules'|'report'|'done'|'error', progress: number, message?: string }`

- **GET** `/results/{jobId}`
  - **Response**:
    ```json
    {
      "overallScore": 94.5,
      "meta": { "fps": 30, "width": 1920, "height": 1080, "is360": false, "modelVersion": "yolov8s-2025-09" },
      "violations": [
        {
          "id": "V-001",
          "type": "missing_helmet",
          "severity": "major",
          "timestampSec": 42.1,
          "bboxPct": [0.32, 0.18, 0.12, 0.22],
          "trackId": 7,
          "confidence": 0.91,
          "evidence": { "thumbUrl": "/evidence/V-001.jpg", "clipUrl": "/clips/V-001.mp4" }
        }
      ],
      "timeline": [ { "timestampSec": 42.1, "markers": [ { "type": "missing_helmet", "severity": "major" } ] } ],
      "detectionsSample": [
        { "frameIdx": 105, "timestampSec": 42.0, "items": [ { "class": "person", "conf": 0.97, "bboxPct": [0.31,0.17,0.14,0.24], "trackId": 7 } ] }
      ]
    }
    ```

**Notes:**
- Percent-based boxes ensure overlay correctness in responsive UI.
- Evidence URLs plug directly into Results and Report pages.

---

## 5) 360° Video Handling (Feasible & Clear)

- Input format: equirectangular (width:height ≈ 2:1).
- Tiling: split horizontally into 6–8 overlapping tiles; project per-tile to planar; detect; map back; merge duplicates on seams by IoU+class.
- Same tracking/rules pipeline after merge; return global `bboxPct`.

---

## 6) What Exists Today vs. What’s Needed

Current repository (`compliance_analysis_notebook.ipynb`) already demonstrates:
- Video I/O, sampling, basic detection (YOLOv8), pose estimation (MediaPipe), simple rule evaluation, and scoring.
- Outputs and visuals that align with your dashboard overlays and scoring.

Additions needed to power the dashboard end-to-end:
- A production AI inference service (FastAPI) implementing the pipeline and the API above.
- Model export and optimization (ONNX/TensorRT) for speed; ByteTrack for tracking.
- Evidence generation (thumbs/clips) and storage layer.
- 360° tiling + merge module.
- Async job orchestration and progress reporting.

---

## 7) Feasibility Matrix (Concise)

| Frontend Claim/Feature | Status with This Workflow |
|---|---|
| Analyze standard MP4 walkthroughs | Feasible now (detector + tracker + rules) |
| Analyze 360° equirectangular videos | Feasible with tiling+merge module |
| PPE detection (helmet, vest, gloves) | Feasible with targeted fine-tuning |
| Exit blockage / obstruction | Feasible with detection + zone masks |
| Ergonomic posture flags | Feasible (pose on person tracks) |
| Evidence thumbnails and clips | Feasible (server-side FFmpeg) |
| Interactive timeline with jump-to | Feasible (based on timestamps) |
| Severity-weighted scoring | Feasible (backend mirrors dashboard logic) |
| Multi-industry rule packs | Feasible (activate per `industry`) |
| Real-time streaming analysis | Not in current workflow; requires stream ingest & low-latency optimizations |
| OCR-based label/sign validation | Not covered here; add OCR (Tesseract/PP-OCR) if required |
| Full scene 3D reconstruction | Out of scope; not required for MVP |

---

## 8) Known Gaps / Not Feasible (as-is)

- True real-time streaming alerts (sub-second) — current plan is batch/near-real-time per video job.
- Advanced OCR for inspection tags/labels — separate OCR pipeline not included yet.
- Complex environment physics or 3D mapping — not required for walkthrough video compliance.
- Predictive analytics — can be added later once historical data accumulates.

---

## 9) Minimal Viable Scope (Recommended)

- Detector classes: `person, helmet, vest, gloves, obstruction, exit_sign, fire_extinguisher, spill`.
- Tracking: ByteTrack on `person`.
- Rules: missing PPE in zones; blocked exits/walkways; basic spills.
- 360°: tiling+merge implementation.
- Evidence: thumbnail + 10s clip per violation.
- API: `/analyze`, `/status/{jobId}`, `/results/{jobId}`.

This scope is unique, scalable, and effective while remaining feasible.

---

## 10) References to This Repo

- Notebook: `compliance_analysis_notebook.ipynb` (prototype pipeline and visuals)
- Outputs: `outputs/` (evidence/reports directory structure you can mirror)
- Rules/SOPs: `rules/`, `sops/` (seed content for the rule engine)

This workflow keeps your dashboard intact and adds a clear, feasible AI backend that supports both standard and 360° videos with a concrete, minimal API. 